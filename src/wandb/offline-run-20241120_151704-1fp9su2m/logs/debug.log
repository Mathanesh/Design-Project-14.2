2024-11-20 15:17:04,549 INFO    MainThread:2710 [wandb_setup.py:_flush():79] Current SDK version is 0.18.7
2024-11-20 15:17:04,549 INFO    MainThread:2710 [wandb_setup.py:_flush():79] Configure stats pid to 2710
2024-11-20 15:17:04,549 INFO    MainThread:2710 [wandb_setup.py:_flush():79] Loading settings from /home/valsamu/.config/wandb/settings
2024-11-20 15:17:04,549 INFO    MainThread:2710 [wandb_setup.py:_flush():79] Loading settings from /home/valsamu/DRL-Traj-Planner/src/wandb/settings
2024-11-20 15:17:04,549 INFO    MainThread:2710 [wandb_setup.py:_flush():79] Loading settings from environment variables: {}
2024-11-20 15:17:04,549 INFO    MainThread:2710 [wandb_setup.py:_flush():79] Applying setup settings: {'mode': None, '_disable_service': None}
2024-11-20 15:17:04,549 INFO    MainThread:2710 [wandb_setup.py:_flush():79] Inferring run settings from compute environment: {'program_relpath': 'src/train.py', 'program_abspath': '/home/valsamu/DRL-Traj-Planner/src/train.py', 'program': '/home/valsamu/DRL-Traj-Planner/src/train.py'}
2024-11-20 15:17:04,549 INFO    MainThread:2710 [wandb_setup.py:_flush():79] Applying login settings: {}
2024-11-20 15:17:04,549 INFO    MainThread:2710 [wandb_setup.py:_flush():79] Applying login settings: {'mode': 'offline'}
2024-11-20 15:17:04,549 INFO    MainThread:2710 [wandb_init.py:_log_setup():533] Logging user logs to /home/valsamu/DRL-Traj-Planner/src/wandb/offline-run-20241120_151704-1fp9su2m/logs/debug.log
2024-11-20 15:17:04,549 INFO    MainThread:2710 [wandb_init.py:_log_setup():534] Logging internal logs to /home/valsamu/DRL-Traj-Planner/src/wandb/offline-run-20241120_151704-1fp9su2m/logs/debug-internal.log
2024-11-20 15:17:04,549 INFO    MainThread:2710 [wandb_init.py:init():619] calling init triggers
2024-11-20 15:17:04,549 INFO    MainThread:2710 [wandb_init.py:init():626] wandb.init called with sweep_config: {}
config: {}
2024-11-20 15:17:04,549 INFO    MainThread:2710 [wandb_init.py:init():669] starting backend
2024-11-20 15:17:04,549 INFO    MainThread:2710 [wandb_init.py:init():673] sending inform_init request
2024-11-20 15:17:04,550 INFO    MainThread:2710 [backend.py:_multiprocessing_setup():104] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2024-11-20 15:17:04,551 INFO    MainThread:2710 [wandb_init.py:init():686] backend started and connected
2024-11-20 15:17:04,554 INFO    MainThread:2710 [wandb_init.py:init():781] updated telemetry
2024-11-20 15:17:04,568 INFO    MainThread:2710 [wandb_init.py:init():814] communicating run to backend with 90.0 second timeout
2024-11-20 15:17:04,701 INFO    MainThread:2710 [wandb_init.py:init():867] starting run threads in backend
2024-11-20 15:17:04,759 INFO    MainThread:2710 [wandb_run.py:_console_start():2456] atexit reg
2024-11-20 15:17:04,759 INFO    MainThread:2710 [wandb_run.py:_redirect():2305] redirect: wrap_raw
2024-11-20 15:17:04,759 INFO    MainThread:2710 [wandb_run.py:_redirect():2370] Wrapping output streams.
2024-11-20 15:17:04,759 INFO    MainThread:2710 [wandb_run.py:_redirect():2395] Redirects installed.
2024-11-20 15:17:04,761 INFO    MainThread:2710 [wandb_init.py:init():911] run started, returning control to user process
2024-11-20 15:17:04,763 INFO    MainThread:2710 [wandb_run.py:_config_callback():1387] config_cb None None {'env_name': 'TrajectoryPlannerEnvironmentRaysReward3-v3', 'reward_mode': 'sum', 'map_key': 'train_2', 'seed': 10, 'collector_device': 'cpu', 'device': 'cpu', 'use_vec_norm': False, 'n_envs': 1, 'w1': 0.1, 'w2': 0.1, 'w3': 0.25, 'w4': 0.1, 'w5': 0.1, 'algo': 'sac', 'sac': {'seed': 10, 'max_eps_steps': 250, 'reset_pretrained_actor': False, 'reward_mode': 'sum', 'curriculum': {'steps_stage_1': 100000, 'reset_n_critic_layers': None, 'reset_n_actor_layers': None, 'reset_buffer': False, 'reset_frames': False, 'num_updates_after_update': 25000, 'base_reward_keys': 'gps', 'all_reward_keys': 'gps'}, 'total_frames': 50000, 'init_random_frames': 5000, 'frames_per_batch': 1000, 'init_env_steps': 5000, 'env_per_collector': 1, 'reset_at_each_iter': False, 'use_multicollector': False, 'scratch_dir': None, 'prefetch': 5, 'hidden_sizes': [32, 32], 'activation': 'relu', 'actor_dropout': None, 'critic_dropout': None, 'default_policy_scale': 1.0, 'scale_lb': 0.1, 'device': 'cpu', 'collector_device': 'cpu', 'gamma': 0.99, 'gamma_end': 0.99, 'weight_decay': 0.0, 'adam_eps': 1e-08, 'max_grad_norm': 1.0, 'loss_function': 'smooth_l1', 'replay_buffer_size': 50000, 'prioritize': False, 'batch_size': 128, 'utd_ratio': 1.0, 'n_reset_layers': None, 'n_reset_layers_critic': None, 'eval_iter': 100000, 'eval_rollout_steps': 5000, 'use_lr_schedule': False, 'first_reduce_frame': 10000, 'eta_min': 1e-06, 'kl_approx_method': 'abs', 'actor_lr': 0.0003, 'critic_lr': 0.0003, 'alpha_lr': 0.0003, 'target_update_polyak': 0.995, 'alpha_init': 1.0, 'min_alpha': 0.01, 'kl_beta': None}, 'ppo': {'seed': 10, 'max_eps_steps': 250, 'reset_pretrained_actor': False, 'reward_mode': 'sum', 'curriculum': {'steps_stage_1': 100000, 'reset_n_critic_layers': None, 'reset_n_actor_layers': None, 'reset_buffer': False, 'reset_frames': False, 'num_updates_after_update': 25000, 'base_reward_keys': 'gps', 'all_reward_keys': 'gps'}, 'total_frames': 50000, 'init_random_frames': 5000, 'frames_per_batch': 1000, 'init_env_steps': 5000, 'env_per_collector': 1, 'reset_at_each_iter': False, 'use_multicollector': False, 'scratch_dir': None, 'prefetch': 5, 'hidden_sizes': [32, 32], 'activation': 'relu', 'actor_dropout': None, 'critic_dropout': None, 'default_policy_scale': 1.0, 'scale_lb': 0.1, 'device': 'cpu', 'collector_device': 'cpu', 'gamma': 0.99, 'gamma_end': 0.99, 'weight_decay': 0.0, 'adam_eps': 1e-08, 'max_grad_norm': 1.0, 'loss_function': 'smooth_l1', 'replay_buffer_size': 1000, 'prioritize': False, 'batch_size': 128, 'utd_ratio': 1.0, 'n_reset_layers': None, 'n_reset_layers_critic': None, 'eval_iter': 100000, 'eval_rollout_steps': 5000, 'use_lr_schedule': False, 'first_reduce_frame': 10000, 'eta_min': 1e-06, 'kl_approx_method': 'abs', 'lr': 0.0003, 'clip_epsilon': 0.2, 'entropy_bonus': True, 'samples_mc_entropy': 1, 'entropy_coef': 0.01, 'critic_coef': 1.0, 'normalize_advantage': False, 'lmbda': 0.95}, 'td3': {'seed': 10, 'max_eps_steps': 250, 'reset_pretrained_actor': False, 'reward_mode': 'sum', 'curriculum': {'steps_stage_1': 100000, 'reset_n_critic_layers': None, 'reset_n_actor_layers': None, 'reset_buffer': False, 'reset_frames': False, 'num_updates_after_update': 25000, 'base_reward_keys': 'gps', 'all_reward_keys': 'gps'}, 'total_frames': 50000, 'init_random_frames': 5000, 'frames_per_batch': 1000, 'init_env_steps': 5000, 'env_per_collector': 1, 'reset_at_each_iter': False, 'use_multicollector': False, 'scratch_dir': None, 'prefetch': 5, 'hidden_sizes': [32, 32], 'activation': 'relu', 'actor_dropout': None, 'critic_dropout': None, 'default_policy_scale': 1.0, 'scale_lb': 0.1, 'device': 'cpu', 'collector_device': 'cpu', 'gamma': 0.99, 'gamma_end': 0.99, 'weight_decay': 0.0, 'adam_eps': 1e-08, 'max_grad_norm': 1.0, 'loss_function': 'smooth_l1', 'replay_buffer_size': 50000, 'prioritize': False, 'batch_size': 128, 'utd_ratio': 1.0, 'n_reset_layers': None, 'n_reset_layers_critic': None, 'eval_iter': 100000, 'eval_rollout_steps': 5000, 'use_lr_schedule': False, 'first_reduce_frame': 10000, 'eta_min': 1e-06, 'kl_approx_method': 'abs', 'actor_lr': 0.0003, 'critic_lr': 0.0003, 'target_update_polyak': 0.995, 'policy_noise': 0.2, 'noise_clip': 0.5, 'sigma_init': 0.9, 'sigmn_end': 0.1}, 'ddpg': {'seed': 10, 'max_eps_steps': 250, 'reset_pretrained_actor': False, 'reward_mode': 'sum', 'curriculum': {'steps_stage_1': 100000, 'reset_n_critic_layers': None, 'reset_n_actor_layers': None, 'reset_buffer': False, 'reset_frames': False, 'num_updates_after_update': 25000, 'base_reward_keys': 'gps', 'all_reward_keys': 'gps'}, 'total_frames': 50000, 'init_random_frames': 5000, 'frames_per_batch': 1000, 'init_env_steps': 5000, 'env_per_collector': 1, 'reset_at_each_iter': False, 'use_multicollector': False, 'scratch_dir': None, 'prefetch': 5, 'hidden_sizes': [32, 32], 'activation': 'relu', 'actor_dropout': None, 'critic_dropout': None, 'default_policy_scale': 1.0, 'scale_lb': 0.1, 'device': 'cpu', 'collector_device': 'cpu', 'gamma': 0.99, 'gamma_end': 0.99, 'weight_decay': 0.0, 'adam_eps': 1e-08, 'max_grad_norm': 1.0, 'loss_function': 'smooth_l1', 'replay_buffer_size': 50000, 'prioritize': False, 'batch_size': 128, 'utd_ratio': 1.0, 'n_reset_layers': None, 'n_reset_layers_critic': None, 'eval_iter': 100000, 'eval_rollout_steps': 5000, 'use_lr_schedule': False, 'first_reduce_frame': 10000, 'eta_min': 1e-06, 'kl_approx_method': 'abs', 'actor_lr': 0.0003, 'critic_lr': 0.0003, 'target_update_polyak': 0.995, 'sigma_init': 0.9, 'sigmn_end': 0.1}, 'meta': {'seed': 10, 'max_eps_steps': 250, 'reset_pretrained_actor': False, 'reward_mode': 'sum', 'curriculum': {'steps_stage_1': 100000, 'reset_n_critic_layers': None, 'reset_n_actor_layers': None, 'reset_buffer': False, 'reset_frames': False, 'num_updates_after_update': 25000, 'base_reward_keys': 'gps', 'all_reward_keys': 'gps'}, 'total_frames': 50000, 'init_random_frames': 5000, 'frames_per_batch': 1000, 'init_env_steps': 5000, 'env_per_collector': 1, 'reset_at_each_iter': False, 'use_multicollector': False, 'scratch_dir': './meta_buffer_test', 'prefetch': 5, 'hidden_sizes': [32, 32], 'activation': 'relu', 'actor_dropout': None, 'critic_dropout': None, 'default_policy_scale': 1.0, 'scale_lb': 0.1, 'device': 'cpu', 'collector_device': 'cpu', 'gamma': 0.99, 'gamma_end': 0.99, 'weight_decay': 0.0, 'adam_eps': 1e-08, 'max_grad_norm': None, 'loss_function': 'smooth_l1', 'replay_buffer_size': 50000, 'prioritize': False, 'batch_size': 128, 'utd_ratio': 10.0, 'n_reset_layers': None, 'n_reset_layers_critic': None, 'eval_iter': 100000, 'eval_rollout_steps': 5000, 'use_lr_schedule': False, 'first_reduce_frame': 10000, 'eta_min': 1e-06, 'kl_approx_method': 'abs', 'actor_lr': 0.0003, 'critic_lr': 0.0003, 'alpha_lr': 0.0001, 'target_update_polyak': 0.995, 'alpha_init': 0.8, 'min_alpha': 0.01, 'kl_beta': None, 'n_iters': 50, 'meta_init_env_steps': 5, 'meta_prioritize': False, 'meta_batch_size': 128, 'meta_replay_buffer_size': 10000, 'meta_hidden_dim': 32, 'meta_cnn_hidden_dim': 16, 'meta_action_ratio': 50, 'meta_reward_scale': 500.0, 'buffer_save_path': './meta_buffer_test', 'num_updates_after_update': 2000}}
2024-11-20 15:17:05,463 INFO    MainThread:2710 [wandb_config.py:__setitem__():154] config set path = /home/valsamu/DRL-Traj-Planner/Model/testing/24_11_20_15_17_05_SAC - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x7f5ed60aab20>>
2024-11-20 15:17:05,463 INFO    MainThread:2710 [wandb_run.py:_config_callback():1387] config_cb path /home/valsamu/DRL-Traj-Planner/Model/testing/24_11_20_15_17_05_SAC None
2024-11-20 15:17:05,463 INFO    MainThread:2710 [wandb_config.py:__setitem__():154] config set map = generate_map_train_2 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x7f5ed60aab20>>
2024-11-20 15:17:05,463 INFO    MainThread:2710 [wandb_run.py:_config_callback():1387] config_cb map generate_map_train_2 None
2024-11-20 15:47:19,191 WARNING MsgRouterThr:2710 [router.py:message_loop():75] message_loop has been closed
